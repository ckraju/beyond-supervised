{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "2-Self-Supervised_Learning_with_Context_Inpainting.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckraju/beyond-supervised/blob/main/2-Self-Supervised_Learning_with_Context_Inpainting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEf9mwSyxkRA"
      },
      "source": [
        "# Context Encoders: Feature Learning by Inpainting\n",
        "### Deepak Pathak, Phillip Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros.\n",
        "### CVPR, 2016\n",
        "<a href=\"http://people.eecs.berkeley.edu/~pathak/papers/cvpr16.pdf\">[Paper]</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q47a4YSDxkRE"
      },
      "source": [
        "<img src=\"http://people.eecs.berkeley.edu/~pathak/context_encoder/resources/teaser.jpg\" width=\"400\"/> <br/>\n",
        "Given an image with a missing region (a), a human artist has no trouble inpainting it (b). Automatic inpainting using our context encoder trained with L2 reconstruction loss is shown in (c), and using both L2 and adversarial losses in (d)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLfHCs5exkRF"
      },
      "source": [
        "A CNN (encoder-decoder network) is trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, the model needs to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s).\n",
        "\n",
        "As also seen in previous notebook, the (self-) supervision in the form of semantic inpainting is obtained with no cost and is very effective in learning useful representations.\n",
        "\n",
        "We will use 5,000 images for pre-training the network for semantic inpainting task and later use this pre-trained model for face parsing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kutLRTbwxkRH"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "### import other stuffs\n",
        "from enc_dec import encoder_decoder\n",
        "from utils import *\n",
        "from inpaint_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "DATA_ROOT = '/tmp/school/data/beyond_supervised/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IAu-aHYJxkRI"
      },
      "source": [
        "### define dataset paths\n",
        "train_img_root = DATA_ROOT + 'part_labels/data/all/'\n",
        "train_image_list = DATA_ROOT + 'part_labels/splits/train_unlabeled_5k.txt'\n",
        "\n",
        "val_img_root = DATA_ROOT + 'part_labels/data/all/'\n",
        "val_image_list = DATA_ROOT + 'part_labels/splits/val_unlabeled_500.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FrW5JyrxkRJ"
      },
      "source": [
        "You can change the amount and size of regions to be erased by passing argument to the data loader. Default is context_shape = [32, 32], context_count = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H9wfqoURxkRK"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(ContextInpaintingDataLoader(img_root = train_img_root,\n",
        "                                                                  image_list = train_image_list, mirror = True),\n",
        "                                           batch_size=16, num_workers=2, shuffle = True, pin_memory=False)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(ContextInpaintingDataLoader(img_root = val_img_root,\n",
        "                                                                  image_list = val_image_list, mirror = True),\n",
        "                                           batch_size=16, num_workers=2, shuffle = False, pin_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Gx7LlJxkRK"
      },
      "source": [
        "We define an encoder-decoder architecture with 4 convolution layers each. Each convolution layer (except the last layer) is followed by BatchNorm and ReLU (not shown in figure). We will use context inpainting technique to pre-train the encoder as well as the decoder in self-supervised way and later use it for face parsing in 3rd notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3sR-jxjxkRM"
      },
      "source": [
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vS_yenRY55ol0M6k3aJTh6yVVSYEgcCmqQEFWtkBeCg2tXOtMLTntjWZgwtrGy4xFitUVs3n-W6Ss5Y/pub?w=2373&h=442\" width=1400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uYxr7UaRxkRN"
      },
      "source": [
        "net = encoder_decoder().cuda()\n",
        "tanh = nn.Tanh()\n",
        "experiment = 'self_supervised_pre_train_semantic_inpainting'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fEEMTUTaxkRO"
      },
      "source": [
        "print('Net params count (M): ', param_counts(net)/(1000000.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kvtu4w-yxkRa"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "best_loss = 9999  # best test accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyeRp-hpxkRc"
      },
      "source": [
        "We use MSE loss for inpainting task. Higher weight (0.99) is applied to loss correspnding to the missing regions, while 0.01 weight is used at other regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ESjaE3dtxkRd"
      },
      "source": [
        "def train(epoch):\n",
        "    print('\\nTrain epoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (inputs, masks, contexts) in enumerate(train_loader):\n",
        "        if use_cuda:\n",
        "            inputs, masks, contexts = inputs.cuda(), masks.cuda(), contexts.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        inputs = Variable(inputs)\n",
        "        masks = Variable(masks)\n",
        "        contexts = Variable(contexts)\n",
        "        outputs = tanh(net(inputs))\n",
        "        loss = 0.99*torch.mean(torch.mul((outputs - contexts)**2, masks)) + 0.01*torch.mean(torch.mul((outputs - contexts)**2, 1-masks))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.data[0]\n",
        "        \n",
        "    print('Loss: %f '% (train_loss/(batch_idx+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "C-vIa4GfxkRe"
      },
      "source": [
        "def val(epoch):\n",
        "    print('\\nVal epoch: %d' % epoch)\n",
        "    global best_loss\n",
        "    net.eval()\n",
        "    val_loss = 0\n",
        "    for batch_idx, (inputs, masks, contexts) in enumerate(val_loader):\n",
        "        if use_cuda:\n",
        "            inputs, masks, contexts = inputs.cuda(), masks.cuda(), contexts.cuda()\n",
        "        inputs = Variable(inputs)\n",
        "        masks = Variable(masks)\n",
        "        contexts = Variable(contexts)\n",
        "        outputs = tanh(net(inputs))\n",
        "        loss = 0.99*torch.mean(torch.mul((outputs - contexts)**2, masks)) + 0.01*torch.mean(torch.mul((outputs - contexts)**2, 1-masks))\n",
        "        val_loss += loss.data[0]\n",
        "        \n",
        "    print('Loss: %f '% (val_loss/(batch_idx+1)))\n",
        "    # Save checkpoint.\n",
        "    if val_loss < best_loss:\n",
        "        print('Saving..')\n",
        "        state = {'net': net}\n",
        "        if not os.path.isdir(DATA_ROOT + 'checkpoint'):\n",
        "            os.mkdir(DATA_ROOT + 'checkpoint')\n",
        "        torch.save(state, DATA_ROOT + 'checkpoint/'+experiment+'ckpt.t7')\n",
        "        best_loss = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "FXY904s6xkRf"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
        "for epoch in range(0, 50):\n",
        "    if epoch == 40:\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "    if epoch == 30:\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "    train(epoch)\n",
        "    val(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HJWcCzSxkRf"
      },
      "source": [
        "Now let's visualize some semantic inpainting results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fd_glyKZxkRg"
      },
      "source": [
        "net = torch.load(DATA_ROOT + 'checkpoint/'+experiment+'ckpt.t7')['net'].cuda().eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1X-AwxqSxkRg"
      },
      "source": [
        "mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n",
        "std_bgr = 255*np.array([0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "NuClHgN7xkRg"
      },
      "source": [
        "val_loader = torch.utils.data.DataLoader(ContextInpaintingDataLoader(img_root = val_img_root,\n",
        "                                                                  image_list = val_image_list, mirror = True),\n",
        "                                           batch_size=1, num_workers=1, shuffle = True, pin_memory=False)\n",
        "\n",
        "for batch_idx, (inputs, masks, contexts) in enumerate(val_loader):\n",
        "    if use_cuda:\n",
        "        inputs, masks, contexts = inputs.cuda(), masks.cuda(), contexts.cuda()\n",
        "    inputs = Variable(inputs)\n",
        "    masks = Variable(masks)\n",
        "    contexts = Variable(contexts)\n",
        "    outputs = tanh(net(inputs))\n",
        "    i = (inputs[0].data.cpu().numpy().transpose(1,2,0) + mean_bgr).astype(np.uint8)[:,:,::-1]\n",
        "    c = (contexts[0].data.cpu().numpy().transpose(1,2,0)*3*std_bgr + mean_bgr).astype(np.uint8)[:,:,::-1]\n",
        "    o = (outputs[0].data.cpu().numpy().transpose(1,2,0)*3*std_bgr + mean_bgr).astype(np.uint8)[:,:,::-1]\n",
        "    vis = np.concatenate((i,c,o), axis = 1)\n",
        "    plt.imshow(vis)\n",
        "    plt.show()\n",
        "    \n",
        "    if batch_idx == 50:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fc3qdDDkxkRh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}