{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Visual Representation Learning by Context Prediction\n",
    "### Carl Doersch, Abhinav Gupta, and Alexei A. Efros.\n",
    "### ICCV, 2015\n",
    "<a href=\"https://arxiv.org/pdf/1505.05192.pdf\">[Paper]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/doersch_1.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are millions of unannotated data available on the web. Can we use these data to effectively learn a useful representation? One such way is unsupervised learning with denoising autoencoder. There are many other such tasks that could lead to better feature learning while incurring no annotation cost. We will look into Context Prediction in this notebook.\n",
    "\n",
    "The idea in this paper is simple. Given two neighboring tiles (indicated with red and blue squares) from an image, the model tries to predict their relative positions. In order to do this task effectively, the model needs to learns the discriminative representations of the patches that constitutes the object. \n",
    "\n",
    "The (self-) supervision in the form of relative position is obtained with no cost and is effective in learning useful representations.\n",
    "\n",
    "We pass each patch through an encoder network (AlexNet, VGG, ResNets, etc.) and get their representations. We then use the concatenated representation of these patches to classify their relative positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will be using <a href=\"http://vis-www.cs.umass.edu/lfw/part_labels/\">Part Labels dataset</a> in this experiment. The task is to label each pixel in the image into one of three classes: Background (blue), Hair (red), and skin (green).\n",
    "<img src=\"http://vis-www.cs.umass.edu/lfw/part_labels/images/img_funneled.jpg\" width=100><img src=\"http://vis-www.cs.umass.edu/lfw/part_labels/images/img_ground_truth.png\" width=100> <br/>\n",
    "There are 13,233 images in total, out of which 2,927 have been labeled. There are 1,500 train, 500 val, and 927 test images. We will be using only 10% of the training set in our experiments. For self-supervised pre-training we will use 5,000 images (available splits are: $ \\tt train\\_unlabeled\\_2k.txt, train\\_unlabeled\\_5k.txt, train\\_unlabeled\\_10k.txt$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "### import other stuffs\n",
    "from enc_dec import encoder\n",
    "from utils import *\n",
    "from relative_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "DATA_ROOT = '/tmp/school/data/beyond_supervised/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### define dataset paths\n",
    "train_img_root = DATA_ROOT + 'part_labels/data/all/'\n",
    "train_image_list = DATA_ROOT + 'part_labels/splits/train_unlabeled_5k.txt'\n",
    "\n",
    "val_img_root = DATA_ROOT + 'part_labels/data/all/'\n",
    "val_image_list = DATA_ROOT + 'part_labels/splits/val_unlabeled_500.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_shape = (64,64)\n",
    "train_loader = torch.utils.data.DataLoader(RelativeTileDataLoader(img_root = train_img_root,\n",
    "                                                                  image_list = train_image_list, crop_shape = crop_shape,mirror = True),\n",
    "                                           batch_size=128, num_workers=2, shuffle = True, pin_memory=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(RelativeTileDataLoader(img_root = val_img_root,\n",
    "                                                                  image_list = val_image_list, crop_shape = crop_shape, mirror = True),\n",
    "                                           batch_size=32, num_workers=2, shuffle = False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an encoder architecture with 4 convolution layers. We will use context prediction technique to pre-train the encoder in self-supervised way and later use it for face parsing in 3rd notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQ8zrtcyVOGwxvd8HgccmSWQad_WKefGT_KDQIu61IcAgzYw-MxfYWgwPKI25mu7etpm2b09jBwoqgj/pub?w=1413&h=360\" width = 1200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = encoder().cuda()\n",
    "experiment = 'self_supervised_pre_train_relative_tile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Net params count (M): ', param_counts(net)/(1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"simple mlp\"\"\"\n",
    "mlp = nn.Sequential(nn.Linear(2048,16),nn.ReLU(),nn.Dropout(0.5),nn.Linear(16,8)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "best_loss = 9999  # best val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "def train(epoch):\n",
    "    print('\\nTrain epoch: %d' % epoch)\n",
    "    net.train()\n",
    "    mlp.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (center_crops, random_crops, class_idxs, class_locs) in enumerate(train_loader):\n",
    "\n",
    "        if use_cuda:\n",
    "            center_crops, random_crops, class_idxs = center_crops.cuda(), random_crops.cuda(), class_idxs.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        center_crops = Variable(center_crops,requires_grad = True)\n",
    "        random_crops = Variable(random_crops,requires_grad = True)\n",
    "        class_idxs = Variable(class_idxs,requires_grad = False)\n",
    "        \n",
    "        v = torch.cat( (net(center_crops).view(center_crops.size()[0],-1),net(random_crops).view(center_crops.size()[0],-1)),1 )\n",
    "\n",
    "        outputs = mlp(v)\n",
    "        loss = loss_fn(outputs,class_idxs)\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "    print('Loss: %f '% (train_loss/(batch_idx+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    print('\\nVal epoch: %d' % epoch)\n",
    "    global best_loss\n",
    "    net.eval()\n",
    "    mlp.eval()\n",
    "    val_loss = 0\n",
    "    for batch_idx, (center_crops, random_crops, class_idxs, class_locs) in enumerate(val_loader):\n",
    "        if use_cuda:\n",
    "            center_crops,random_crops,class_idxs = center_crops.cuda(),random_crops.cuda(),class_idxs.cuda()\n",
    "        center_crops = Variable(center_crops,requires_grad=True)\n",
    "        random_crops = Variable(random_crops,requires_grad=True)\n",
    "        class_idxs = Variable(class_idxs,requires_grad=False)\n",
    "        v = torch.cat((net(center_crops).view(center_crops.size()[0],-1),net(random_crops).view(random_crops.size()[0],-1)),1)\n",
    "        outputs = mlp(v)\n",
    "        loss = loss_fn(outputs,class_idxs)\n",
    "        val_loss += loss.data[0]\n",
    "        \n",
    "    print('Loss: %f '% (val_loss/(batch_idx+1)))\n",
    "    # Save checkpoint.\n",
    "    if val_loss < best_loss:\n",
    "        print('Saving..')\n",
    "        state = {'net': net}\n",
    "        if not os.path.isdir(DATA_ROOT + 'checkpoint'):\n",
    "            os.mkdir(DATA_ROOT + 'checkpoint')\n",
    "        torch.save(state, DATA_ROOT + 'checkpoint/'+experiment+'ckpt.t7')\n",
    "        best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(list(net.parameters()) + list(mlp.parameters()), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "for epoch in range(0, 100):\n",
    "    if epoch == 80:\n",
    "        optimizer = optim.SGD(list(net.parameters()) + list(mlp.parameters()), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "    if epoch == 60:\n",
    "        optimizer = optim.SGD(list(net.parameters()) + list(mlp.parameters()), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "    train(epoch)\n",
    "    val(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
